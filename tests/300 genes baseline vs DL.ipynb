{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto(device_count = {'CPU': 1}, intra_op_parallelism_threads=7, inter_op_parallelism_threads=1)\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "import keras\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "import PconvNetPolimi as pcnp\n",
    "import numpy as np\n",
    "import gene4cancer as gc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 20.0)\n",
    "matplotlib.rcParams['font.size'] = 22\n",
    "matplotlib.rcParams['xtick.labelsize'] = 22\n",
    "matplotlib.rcParams['ytick.labelsize'] = 22\n",
    "matplotlib.rcParams['axes.labelsize'] = 30\n",
    "matplotlib.rcParams['axes.titlesize'] = 30\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dm = '/home/nanni/Data/HiC/50kb/distance_matrix.npy'\n",
    "d_m = np.load(path_dm)\n",
    "idx_to_gene_path = '/home/nanni/Data/HiC/50kb/idx_to_gene.csv'\n",
    "df_idx_to_gene = pd.read_csv(idx_to_gene_path,index_col=0,header = None)\n",
    "gene_to_idx_path = '/home/nanni/Data/HiC/50kb/gene_to_idx.csv'\n",
    "df_gene_to_idx = pd.read_csv(gene_to_idx_path,index_col=0,header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakalouski/.conda/envs/artur/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "path_data = '/home/nanni/Data/TCGA/tcga.tsv'\n",
    "df_data = pd.read_csv(path_data,sep='\\t')\n",
    "filtered_df = df_data[df_data['sample_type'].isin(['Primary Solid Tumor','Solid Tissue Normal'])]\n",
    "gene_dm = df_gene_to_idx.index.tolist()\n",
    "X_normal, Y_normal, X_tumor, Y_tumor, gene_to_idx, idx_to_gene = gc.data_preprocessing.preprocess_data(filtered_df[filtered_df.columns[:7].tolist() + gene_dm])\n",
    "X, Y, state = gc.data_preprocessing.assemble_normal_tumor(X_normal, Y_normal, X_tumor, Y_tumor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking 300 random genes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = list(np.random.randint(low=0, high=X.shape[1], size = (300)))\n",
    "X_300 = X[:,indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = gc.data_preprocessing.split(X_300, Y)\n",
    "#X_train, Y_train, X_cv, Y_cv = gc.data_preprocessing.split(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BRCA       1.00      0.99      1.00       353\n",
      "   COADREAD       1.00      0.98      0.99       124\n",
      "       KIRC       1.00      0.96      0.98       186\n",
      "       LIHC       1.00      0.98      0.99       129\n",
      "       LUAD       0.07      0.05      0.06       164\n",
      "       LUNG       0.26      0.37      0.31       329\n",
      "       LUSC       0.00      0.00      0.00       172\n",
      "       PRAD       1.00      0.99      1.00       175\n",
      "       THCA       1.00      0.99      1.00       179\n",
      "\n",
      "avg / total       0.69      0.70      0.69      1811\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "classy = SVC()\n",
    "classy.fit(X_train,Y_train)\n",
    "y_pred = classy.predict(X_test)\n",
    "rep = classification_report(Y_test, y_pred)\n",
    "print(rep)\n",
    "#print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for DL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 61, 178, 224, 196, 1, 17, 187, 60, 15, 2, 235, 68, 288, 154, 3, 192, 52, 143, 19, 4, 102, 119, 194, 62, 5, 14, 97, 82, 139, 6, 138, 105, 149, 242, 7, 106, 267, 72, 188, 8, 81, 101, 268, 297, 9, 100, 7, 260, 44, 10, 4, 78, 97, 139, 11, 174, 246, 232, 210, 12, 0, 224, 178, 73, 13, 85, 144, 64, 83, 14, 5, 82, 97, 64, 15, 146, 105, 165, 151, 16, 102, 4, 62, 226, 17, 255, 169, 201, 41, 18, 172, 202, 230, 197, 19, 193, 202, 99, 80, 20, 207, 98, 141, 244, 21, 117, 87, 274, 67, 22, 45, 256, 95, 136, 23, 181, 110, 226, 7, 24, 83, 188, 170, 131, 25, 97, 188, 78, 89, 26, 262, 72, 191, 267, 27, 6, 105, 146, 31, 28, 207, 98, 296, 297, 29, 282, 7, 49, 195, 30, 100, 49, 91, 136, 31, 216, 16, 181, 269, 32, 33, 158, 60, 150, 33, 32, 239, 89, 150, 34, 157, 223, 295, 19, 35, 53, 171, 225, 163, 36, 271, 94, 211, 280, 37, 168, 293, 51, 270, 38, 288, 203, 154, 68, 39, 45, 199, 5, 166, 40, 24, 83, 188, 170, 41, 17, 19, 23, 146, 42, 254, 250, 244, 198, 43, 159, 36, 280, 126, 44, 251, 116, 86, 221, 45, 95, 256, 39, 144, 46, 129, 142, 192, 289, 47, 266, 63, 277, 141, 48, 53, 24, 40, 225, 49, 60, 209, 139, 97, 50, 215, 179, 132, 272, 51, 112, 204, 37, 220, 52, 3, 130, 185, 213, 53, 35, 48, 144, 249, 54, 255, 201, 169, 259, 55, 123, 225, 134, 269, 56, 174, 232, 238, 254, 57, 178, 73, 61, 224, 58, 208, 217, 165, 105, 59, 53, 219, 123, 264, 60, 247, 91, 76, 291, 61, 0, 196, 57, 178, 62, 102, 119, 269, 4, 63, 47, 104, 266, 277, 64, 14, 158, 195, 5, 65, 203, 202, 201, 200, 66, 243, 139, 24, 31, 67, 180, 187, 117, 21, 68, 2, 288, 103, 203, 69, 92, 211, 283, 94, 70, 85, 226, 299, 260, 71, 214, 282, 287, 79, 72, 273, 176, 26, 262, 73, 57, 12, 196, 296, 74, 289, 129, 142, 212, 75, 34, 157, 226, 8, 76, 82, 247, 158, 60, 77, 203, 202, 201, 200, 78, 59, 10, 170, 53, 79, 214, 192, 237, 71, 80, 230, 140, 86, 19, 81, 101, 8, 239, 19, 82, 14, 76, 195, 291, 83, 40, 53, 251, 188, 84, 41, 269, 232, 256, 85, 118, 238, 70, 248, 86, 90, 257, 197, 279, 87, 21, 117, 9, 296, 88, 297, 14, 138, 218, 89, 33, 60, 233, 166, 90, 257, 197, 279, 86, 91, 60, 139, 239, 97, 92, 283, 162, 69, 240, 93, 246, 11, 232, 127, 94, 36, 182, 271, 159, 95, 45, 22, 227, 226, 96, 217, 213, 134, 72, 97, 5, 195, 124, 276, 98, 206, 227, 207, 20, 99, 116, 19, 140, 86, 100, 188, 295, 9, 251, 101, 81, 8, 168, 229, 102, 62, 194, 4, 16, 103, 234, 154, 156, 68, 104, 63, 277, 141, 296, 105, 286, 6, 27, 15, 106, 7, 251, 208, 242, 107, 250, 285, 299, 237, 108, 248, 298, 267, 133, 109, 259, 50, 179, 96, 110, 23, 39, 199, 269, 111, 36, 182, 280, 159, 112, 204, 293, 245, 168, 113, 159, 155, 36, 211, 114, 88, 226, 297, 85, 115, 98, 296, 207, 226, 116, 44, 52, 282, 87, 117, 87, 67, 187, 274, 118, 85, 168, 82, 231, 119, 16, 226, 62, 4, 120, 10, 163, 31, 23, 121, 190, 135, 72, 247, 122, 281, 249, 286, 176, 123, 55, 249, 59, 53, 124, 76, 209, 82, 247, 125, 208, 217, 165, 105, 126, 211, 280, 182, 43, 127, 163, 93, 7, 145, 128, 174, 184, 232, 97, 129, 198, 74, 107, 289, 130, 185, 214, 289, 74, 131, 276, 97, 139, 239, 132, 177, 50, 179, 259, 133, 252, 279, 257, 86, 134, 96, 153, 208, 258, 135, 121, 229, 242, 173, 136, 83, 243, 9, 209, 137, 221, 202, 252, 86, 138, 286, 59, 214, 23, 139, 14, 195, 49, 131, 140, 80, 236, 99, 86, 141, 296, 266, 47, 104, 142, 198, 74, 107, 289, 143, 3, 192, 267, 237, 144, 53, 225, 13, 40, 145, 7, 251, 209, 127, 146, 105, 15, 27, 222, 147, 218, 273, 262, 119, 148, 92, 271, 155, 40, 149, 286, 150, 5, 289, 150, 195, 82, 60, 14, 151, 23, 136, 66, 181, 152, 31, 189, 286, 192, 153, 58, 125, 134, 242, 154, 203, 288, 235, 2, 155, 113, 126, 205, 265, 156, 288, 38, 226, 103, 157, 75, 34, 223, 267, 158, 291, 76, 64, 247, 159, 271, 36, 43, 280, 160, 228, 189, 289, 24, 161, 297, 14, 138, 218, 162, 283, 240, 155, 271, 163, 127, 44, 102, 97, 164, 18, 230, 19, 173, 165, 247, 226, 102, 150, 166, 89, 158, 49, 25, 167, 166, 36, 226, 137, 168, 245, 220, 204, 179, 169, 17, 54, 201, 19, 170, 24, 40, 78, 188, 171, 150, 19, 261, 9, 172, 221, 18, 189, 257, 173, 185, 233, 164, 45, 174, 11, 128, 56, 232, 175, 282, 186, 222, 212, 176, 191, 72, 262, 267, 177, 272, 215, 179, 50, 178, 0, 57, 160, 12, 179, 50, 259, 132, 9, 180, 67, 87, 274, 9, 181, 216, 194, 16, 31, 182, 211, 94, 36, 271, 183, 127, 226, 9, 49, 184, 128, 232, 237, 78, 185, 143, 198, 142, 129, 186, 285, 29, 237, 175, 187, 260, 117, 251, 1, 188, 291, 251, 91, 100, 189, 172, 252, 197, 279, 190, 189, 154, 10, 15, 191, 267, 262, 26, 292, 192, 222, 46, 198, 3, 193, 80, 221, 133, 86, 194, 4, 66, 226, 62, 195, 14, 97, 82, 64, 196, 61, 0, 57, 12, 197, 252, 86, 279, 257, 198, 289, 142, 129, 192, 199, 273, 262, 39, 248, 200, 96, 226, 230, 199, 201, 54, 169, 241, 255, 202, 19, 137, 221, 230, 203, 288, 38, 234, 154, 204, 112, 293, 168, 245, 205, 271, 126, 155, 118, 206, 207, 98, 296, 267, 207, 206, 98, 20, 296, 208, 258, 242, 217, 213, 209, 291, 49, 5, 64, 210, 246, 232, 11, 174, 211, 36, 126, 182, 271, 212, 282, 186, 285, 74, 213, 96, 217, 208, 258, 214, 74, 71, 130, 192, 215, 259, 50, 141, 277, 216, 181, 4, 194, 31, 217, 208, 134, 213, 258, 218, 147, 262, 191, 64, 219, 55, 59, 78, 53, 220, 168, 293, 51, 245, 221, 172, 86, 137, 193, 222, 192, 71, 130, 250, 223, 34, 157, 256, 100, 224, 12, 73, 0, 57, 225, 144, 59, 264, 55, 226, 119, 297, 194, 16, 227, 95, 166, 296, 154, 228, 160, 44, 204, 21, 229, 53, 225, 123, 72, 230, 197, 80, 252, 257, 231, 102, 226, 9, 212, 232, 246, 93, 210, 11, 233, 32, 239, 60, 89, 234, 288, 103, 203, 267, 235, 2, 288, 154, 203, 236, 19, 86, 172, 99, 237, 175, 250, 186, 282, 238, 85, 253, 261, 181, 239, 33, 60, 91, 49, 240, 94, 92, 36, 159, 241, 255, 201, 169, 125, 242, 153, 217, 106, 6, 243, 216, 297, 31, 66, 244, 198, 214, 42, 192, 245, 10, 220, 278, 51, 246, 232, 210, 11, 136, 247, 76, 14, 291, 64, 248, 108, 85, 97, 199, 249, 264, 123, 122, 165, 250, 107, 254, 42, 222, 251, 163, 188, 100, 83, 252, 133, 197, 257, 86, 253, 13, 118, 248, 238, 254, 42, 299, 74, 214, 255, 169, 17, 241, 203, 256, 22, 45, 223, 53, 257, 90, 279, 252, 152, 258, 217, 134, 213, 253, 259, 215, 179, 272, 132, 260, 9, 274, 70, 259, 261, 106, 203, 195, 171, 262, 26, 147, 176, 199, 263, 96, 62, 256, 230, 264, 48, 144, 53, 249, 265, 159, 182, 211, 111, 266, 141, 277, 296, 63, 267, 262, 176, 191, 7, 268, 101, 8, 218, 256, 269, 62, 102, 16, 31, 270, 204, 245, 233, 220, 271, 205, 111, 211, 94, 272, 50, 177, 259, 109, 273, 72, 147, 267, 292, 274, 21, 180, 260, 117, 275, 203, 202, 201, 200, 276, 131, 89, 97, 291, 277, 296, 266, 47, 104, 278, 245, 214, 250, 226, 279, 252, 257, 90, 133, 280, 182, 211, 36, 126, 281, 153, 273, 201, 176, 282, 285, 175, 29, 71, 283, 92, 159, 162, 211, 284, 203, 202, 201, 200, 285, 186, 212, 282, 107, 286, 105, 138, 91, 102, 287, 71, 244, 214, 250, 288, 203, 234, 156, 154, 289, 74, 198, 130, 192, 290, 145, 62, 203, 151, 291, 158, 82, 14, 60, 292, 273, 267, 191, 262, 293, 204, 112, 37, 220, 294, 157, 16, 258, 140, 295, 34, 100, 181, 166, 296, 277, 63, 20, 104, 297, 243, 62, 102, 194, 298, 253, 85, 226, 15, 299, 107, 254, 285, 250]\n",
      "[141, 188, 61, 109, 299, 59, 226, 187, 166, 220, 174, 124, 150, 32, 72, 73, 39, 13, 243, 246, 36, 59, 291, 251, 212, 253, 277, 275, 36, 212, 128, 187, 198, 188, 226, 274, 144, 69, 136, 62, 198, 154, 17, 14, 282, 264, 246, 7, 72, 11, 295, 195, 214, 83, 155, 93, 147, 98, 184, 101, 79, 0, 139, 125, 203, 175, 179, 6, 285, 296, 10, 126, 256, 45, 236, 16, 256, 42, 98, 249, 296, 214, 9, 289, 96, 106, 80, 206, 71, 173, 45, 57, 210, 103, 90, 11, 14, 296, 239, 78, 47, 247, 204, 39, 199, 87, 57, 259, 203, 102, 118, 86, 110, 15, 291, 193, 252, 50, 152, 19, 130, 160, 97, 211, 49, 273, 59, 181, 117, 82, 34, 252, 31, 34, 40, 168, 62, 286, 8, 259, 193, 54, 210, 35, 271, 97, 194, 121, 288, 227, 11, 38, 204, 15, 165, 53, 113, 15, 235, 199, 165, 14, 97, 70, 159, 293, 133, 195, 56, 257, 282, 212, 8, 13, 83, 213, 285, 53, 8, 7, 260, 181, 164, 78, 67, 147, 257, 42, 170, 248, 97, 116, 91, 97, 194, 182, 185, 192, 226, 251, 239, 203, 142, 170, 16, 244, 289, 271, 196, 159, 192, 288, 282, 219, 61, 66, 163, 283, 217, 293, 123, 214, 233, 78, 218, 93, 285, 96, 296, 198, 298, 64, 31, 242, 222, 269, 10, 102, 167, 202, 166, 186, 111, 256, 141, 22, 86, 224, 264, 192, 94, 76, 282, 132, 293, 269, 7, 51, 129, 62, 74, 17, 288, 250, 215, 78, 129, 85, 149, 261, 149, 209, 250, 291, 91, 213, 129, 91, 19, 247, 207, 131, 131, 266, 259, 14, 17, 172, 76, 179, 237, 211, 134, 268, 288, 14, 299, 128, 296, 43, 267, 172, 188, 178, 281, 251, 125, 155, 225, 276, 50, 29, 172, 26, 234, 2, 46, 31, 85, 289, 156, 157, 19, 90, 3, 245, 95, 194, 24, 237, 232, 107, 262, 255, 237, 4, 119, 271, 12, 82, 245, 48, 9, 86, 83, 98, 218, 236, 262, 249, 124, 205, 209, 242, 55, 42, 232, 102, 208, 60, 19, 287, 142, 197, 248, 100, 105, 242, 140, 118, 44, 269, 1, 100, 33, 281, 60, 192, 150, 78, 89, 244, 62, 252, 76, 89, 53, 21, 169, 178, 137, 170, 231, 257, 82, 284, 203, 297, 202, 14, 9, 36, 197, 243, 191, 86, 19, 217, 267, 24, 45, 145, 4, 112, 106, 88, 49, 64, 9, 176, 279, 76, 135, 44, 202, 211, 161, 227, 64, 139, 220, 59, 163, 145, 204, 199, 25, 296, 252, 126, 226, 267, 118, 219, 222, 2, 115, 272, 181, 277, 191, 43, 2, 252, 208, 49, 127, 183, 68, 297, 85, 174, 214, 7, 19, 288, 190, 86, 138, 203, 0, 83, 74, 47, 192, 235, 20, 212, 52, 288, 125, 51, 226, 82, 24, 72, 168, 226, 265, 245, 22, 9, 298, 247, 90, 274, 137, 97, 201, 79, 270, 147, 195, 4, 99, 243, 91, 105, 94, 259, 158, 253, 207, 291, 232, 98, 131, 178, 253, 19, 138, 65, 230, 14, 150, 246, 49, 262, 223, 199, 5, 86, 70, 54, 29, 60, 210, 159, 277, 100, 250, 144, 91, 64, 232, 158, 50, 282, 32, 97, 111, 277, 102, 230, 153, 272, 228, 9, 119, 266, 184, 227, 100, 17, 221, 67, 195, 266, 21, 40, 278, 55, 224, 208, 151, 185, 105, 246, 130, 77, 185, 123, 120, 277, 27, 171, 36, 223, 289, 41, 180, 178, 87, 163, 174, 80, 9, 194, 191, 162, 217, 71, 31, 109, 139, 192, 235, 40, 162, 297, 51, 14, 94, 210, 282, 97, 133, 52, 25, 133, 179, 295, 253, 188, 249, 84, 273, 186, 168, 82, 285, 199, 71, 50, 280, 104, 49, 182, 80, 248, 203, 97, 116, 74, 102, 81, 16, 44, 153, 131, 157, 221, 179, 262, 279, 133, 127, 29, 173, 296, 180, 3, 156, 226, 257, 151, 2, 232, 5, 240, 73, 179, 62, 156, 199, 130, 175, 224, 87, 232, 256, 134, 21, 271, 140, 118, 182, 246, 40, 202, 262, 10, 71, 169, 211, 86, 64, 74, 168, 211, 83, 15, 258, 260, 159, 293, 57, 130, 24, 203, 226, 216, 110, 223, 20, 53, 260, 234, 60, 154, 299, 43, 40, 168, 4, 159, 60, 28, 138, 23, 174, 257, 98, 203, 280, 102, 139, 14, 198, 114, 297, 80, 244, 133, 82, 147, 267, 286, 287, 98, 103, 95, 277, 129, 228, 282, 3, 60, 179, 264, 11, 189, 297, 220, 207, 141, 16, 294, 158, 257, 117, 144, 271, 242, 251, 215, 254, 220, 175, 36, 214, 289, 247, 271, 47, 226, 96, 87, 198, 192, 40, 258, 205, 206, 247, 248, 101, 18, 217, 273, 174, 22, 189, 178, 242, 16, 226, 195, 279, 33, 121, 105, 201, 111, 45, 53, 154, 200, 233, 138, 212, 100, 134, 72, 232, 106, 269, 238, 189, 15, 182, 169, 216, 66, 96, 186, 173, 292, 74, 108, 150, 26, 89, 251, 166, 158, 153, 62, 136, 154, 5, 60, 146, 91, 92, 214, 85, 148, 190, 4, 258, 195, 146, 127, 21, 5, 189, 72, 225, 126, 214, 19, 16, 221, 6, 50, 107, 280, 10, 296, 74, 89, 119, 70, 102, 225, 269, 208, 154, 207, 165, 296, 188, 160, 89, 23, 297, 198, 123, 180, 251, 19, 86, 36, 50, 127, 105, 189, 249, 21, 262, 297, 192, 229, 39, 134, 37, 211, 57, 230, 4, 62, 4, 92, 57, 134, 74, 82, 243, 19, 122, 204, 239, 186, 202, 266, 239, 285, 23, 201, 291, 5, 38, 273, 257, 58, 295, 0, 23, 213, 104, 157, 274, 128, 85, 255, 164, 267, 59, 216, 225, 196, 76, 81, 62, 188, 18, 254, 94, 209, 105, 104, 299, 152, 241, 139, 203, 12, 267, 279, 130, 16, 192, 105, 250, 196, 74, 265, 260, 35, 14, 274, 85, 142, 279, 177, 102, 200, 222, 20, 0, 208, 271, 218, 272, 44, 66, 54, 31, 64, 5, 172, 53, 191, 239, 105, 86, 288, 226, 166, 72, 250, 45, 226, 273, 276, 291, 201, 263, 181, 144, 85, 292, 195, 176, 191, 256, 144, 168, 226, 230, 160, 97, 230, 255, 259, 225, 218, 296, 16, 268, 232, 112, 247, 157, 251, 234, 204, 34, 42, 150, 153, 145, 186, 211, 181, 34, 107, 208, 60, 250, 85, 36, 238, 288, 63, 182, 11, 88, 24, 254, 249, 194, 7, 197, 37, 165, 224, 101, 194, 203, 117, 297, 213, 45, 262, 255, 257, 36, 41, 248, 206, 163, 269, 176, 204, 89, 141, 191, 86, 97, 126, 104, 221, 170, 32, 171, 78, 201, 20, 217, 126, 176, 67, 230, 267, 53, 132, 172, 15, 112, 58, 53, 123, 247, 107, 140, 142, 267, 226, 237, 107, 264, 176, 229, 81, 245, 27, 195, 203, 188, 49, 207, 23, 211, 166, 62, 57, 188, 64, 259, 233, 12, 230, 0, 232, 44, 193, 8, 289, 46, 71, 3, 117, 204, 267, 1, 234, 239, 283, 154, 168, 290, 23, 222, 231, 102, 221, 83, 61, 92, 119, 55, 31, 92, 277, 198, 97, 165, 192, 72, 293, 251, 169, 292, 99, 201, 203, 103, 169, 19, 182, 157, 6, 11, 197, 36, 53, 291, 143, 60, 26, 82, 104, 10, 129, 9, 201, 289, 68, 286, 286, 238, 100, 245, 146, 66, 182, 155, 171, 213, 214, 240, 257, 31, 72, 185, 33, 7, 201, 47, 126, 285, 194, 203, 245, 217, 282, 267, 202, 192, 71, 177, 19, 59, 201, 215, 159, 94, 141, 102, 278, 280, 225, 142, 181, 289, 97, 14, 94, 215, 279, 143, 262, 108, 283, 63, 245, 34, 95, 200, 143, 38, 209, 169, 187, 7, 217, 61, 241, 36, 244, 258, 60, 233, 87, 258, 179, 134, 63, 117, 270, 159, 119, 232, 226, 68, 64, 9, 238, 296, 256, 254, 211, 250, 107, 154, 181, 211, 237, 177, 200, 226, 258, 163, 73, 96, 288, 254, 136, 158, 276, 132, 51, 53, 200, 9, 96, 49, 139, 8, 24, 140, 197, 201, 137, 103, 26, 92, 99, 123, 267, 187, 154, 53, 207, 99, 280, 6, 68, 62, 273, 196, 271, 220, 86, 216, 30, 106, 209, 255, 86, 205, 137, 166, 198, 279, 31, 214, 283, 100, 202, 86, 220, 144, 237, 229, 271, 52, 63, 93, 159, 197, 138, 75, 136, 175, 69, 17, 53, 176, 274, 262, 262, 73, 155, 286, 222, 242, 95, 218, 67, 259, 7, 202, 60, 122, 36, 12, 112, 291, 197, 162, 226, 296, 158, 261, 4, 127, 37, 113, 217, 256, 154, 223, 221, 261, 226, 50, 5, 117, 252, 150, 272, 155, 12, 252, 31, 63, 0, 213, 252, 289, 116, 267, 41, 141, 146, 48, 280, 80, 203, 75, 139, 260, 240, 136, 33, 151, 188, 82, 286, 56, 27, 259, 18, 101, 266, 208, 49, 48, 90, 13, 135, 39, 132, 250, 241, 76, 55]\n"
     ]
    }
   ],
   "source": [
    "kern_size = 5\n",
    "d_m_cropped = d_m[indexes,:]\n",
    "d_m_cropped = d_m_cropped[:,indexes]\n",
    "\n",
    "order = []\n",
    "for i in range(X_train.shape[1]):\n",
    "    sortet = np.argsort(d_m_cropped[i,:])\n",
    "    order.append(i)\n",
    "    order.extend(sortet[1:kern_size])\n",
    "\n",
    "print(order)\n",
    "from random import shuffle\n",
    "shuffle(order) #for random dm\n",
    "print(order)\n",
    "X_dl = X_300[:,order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6035, 1500)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def str_to_int(Y):\n",
    "    list_of_tissues = []\n",
    "    for i in range(Y.shape[0]):\n",
    "        e = Y[i]\n",
    "        if e in list_of_tissues:\n",
    "            Y[i] = list_of_tissues.index(e)\n",
    "        else:\n",
    "            list_of_tissues.append(e)\n",
    "            Y[i] = list_of_tissues.index(e)\n",
    "    Y = Y.astype(int)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = str_to_int(Y)\n",
    "X_train_dl, Y_train_dl, X_test_dl, Y_test_dl = gc.data_preprocessing.split(X_dl, Y)\n",
    "\n",
    "Y_input_test = keras.utils.to_categorical(Y_test_dl, num_classes=len(np.unique(Y_test_dl)))\n",
    "Y_input_train = keras.utils.to_categorical(Y_train_dl, num_classes=len(np.unique(Y_train_dl)))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_dl = scaler.fit_transform(X_train_dl)\n",
    "X_test_dl = scaler.transform(X_test_dl)\n",
    "\n",
    "X_input_test = X_test_dl.reshape(X_test_dl.shape[0],X_test_dl.shape[1],-1)\n",
    "X_input_train = X_train_dl.reshape(X_train_dl.shape[0],X_train_dl.shape[1],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input_train, Y_input_train = pcnp.upsampling(X_input_train, Y_input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8]),\n",
       " array([307, 858, 798, 381, 419, 292, 410, 385, 374]))"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y_train_dl, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import (Lambda, Flatten, Dropout, Dense, Input, BatchNormalization)\n",
    "from keras.backend import floatx\n",
    "def create_conv_model(X_train, Y_train, nb_filters, nb_neighbors, opt=None):\n",
    "    \n",
    "    nb_features = X_train.shape[1]\n",
    "    data = Input(shape=(nb_features, 1), name=\"data\", dtype=floatx())\n",
    "    conv_layer = keras.layers.Conv1D(nb_neighbors, nb_filters, activation='relu', strides = nb_neighbors)(data)\n",
    "    #conv_layer = keras.layers.Conv1D(nb_neighbors, nb_filters, activation='relu', strides = nb_neighbors)(conv_layer)\n",
    "    flatt = Flatten()(conv_layer)\n",
    "    drop = Dropout(0.25)(flatt)\n",
    "    drop = Dense(units=16, activation='relu')(drop)\n",
    "    drop = BatchNormalization()(drop)\n",
    "    #drop = Dropout(0.25)(drop)\n",
    "    output = Dense(units=Y_train.shape[1], activation=\"softmax\", name='output')(drop)\n",
    "\n",
    "    model = Model(inputs=data, outputs=output)\n",
    "    from keras import optimizers\n",
    "    opt = optimizers.Nadam(lr=1e-3)\n",
    "    #opt = optimizers.SGD(lr = 1e-4)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "model_name = 'Test'\n",
    "model = create_conv_model(X_input_train, Y_input_train, 3, kern_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5406 samples, validate on 2317 samples\n",
      "Epoch 1/300\n",
      "5100/5406 [===========================>..] - ETA: 0s - loss: 1.0367Epoch 00000: val_loss improved from inf to 0.62425, saving model to Test.h5\n",
      "5406/5406 [==============================] - 2s - loss: 1.0106 - val_loss: 0.6243\n",
      "Epoch 2/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.4832Epoch 00001: val_loss improved from 0.62425 to 0.43784, saving model to Test.h5\n",
      "5406/5406 [==============================] - 0s - loss: 0.4770 - val_loss: 0.4378\n",
      "Epoch 3/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.3842Epoch 00002: val_loss improved from 0.43784 to 0.37477, saving model to Test.h5\n",
      "5406/5406 [==============================] - 0s - loss: 0.3835 - val_loss: 0.3748\n",
      "Epoch 4/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.3362Epoch 00003: val_loss improved from 0.37477 to 0.36071, saving model to Test.h5\n",
      "5406/5406 [==============================] - 0s - loss: 0.3350 - val_loss: 0.3607\n",
      "Epoch 5/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.3132Epoch 00004: val_loss improved from 0.36071 to 0.31613, saving model to Test.h5\n",
      "5406/5406 [==============================] - 0s - loss: 0.3116 - val_loss: 0.3161\n",
      "Epoch 6/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2927Epoch 00005: val_loss improved from 0.31613 to 0.30799, saving model to Test.h5\n",
      "5406/5406 [==============================] - 0s - loss: 0.2922 - val_loss: 0.3080\n",
      "Epoch 7/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2774Epoch 00006: val_loss improved from 0.30799 to 0.30621, saving model to Test.h5\n",
      "5406/5406 [==============================] - 0s - loss: 0.2758 - val_loss: 0.3062\n",
      "Epoch 8/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2673Epoch 00007: val_loss did not improve\n",
      "5406/5406 [==============================] - 0s - loss: 0.2673 - val_loss: 0.3102\n",
      "Epoch 9/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2568Epoch 00008: val_loss did not improve\n",
      "5406/5406 [==============================] - 0s - loss: 0.2576 - val_loss: 0.3126\n",
      "Epoch 10/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2539Epoch 00009: val_loss did not improve\n",
      "5406/5406 [==============================] - 0s - loss: 0.2525 - val_loss: 0.3136\n",
      "Epoch 11/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2520Epoch 00010: val_loss improved from 0.30621 to 0.30170, saving model to Test.h5\n",
      "5406/5406 [==============================] - 0s - loss: 0.2484 - val_loss: 0.3017\n",
      "Epoch 12/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2477Epoch 00011: val_loss did not improve\n",
      "5406/5406 [==============================] - 0s - loss: 0.2448 - val_loss: 0.3167\n",
      "Epoch 13/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2365Epoch 00012: val_loss did not improve\n",
      "5406/5406 [==============================] - 0s - loss: 0.2390 - val_loss: 0.3455\n",
      "Epoch 14/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2414Epoch 00013: val_loss improved from 0.30170 to 0.29855, saving model to Test.h5\n",
      "5406/5406 [==============================] - 0s - loss: 0.2443 - val_loss: 0.2985\n",
      "Epoch 15/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2297Epoch 00014: val_loss did not improve\n",
      "5406/5406 [==============================] - 0s - loss: 0.2309 - val_loss: 0.3058\n",
      "Epoch 16/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2350Epoch 00015: val_loss did not improve\n",
      "5406/5406 [==============================] - 0s - loss: 0.2351 - val_loss: 0.3047\n",
      "Epoch 17/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2306Epoch 00016: val_loss did not improve\n",
      "5406/5406 [==============================] - 0s - loss: 0.2287 - val_loss: 0.3040\n",
      "Epoch 18/300\n",
      "5200/5406 [===========================>..] - ETA: 0s - loss: 0.2231Epoch 00017: val_loss did not improve\n",
      "5406/5406 [==============================] - 0s - loss: 0.2248 - val_loss: 0.3011\n",
      "Epoch 19/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2262Epoch 00018: val_loss did not improve\n",
      "5406/5406 [==============================] - 0s - loss: 0.2231 - val_loss: 0.3087\n",
      "Epoch 20/300\n",
      "5200/5406 [===========================>..] - ETA: 0s - loss: 0.2221Epoch 00019: val_loss did not improve\n",
      "5406/5406 [==============================] - 0s - loss: 0.2222 - val_loss: 0.3035\n",
      "Epoch 21/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2219Epoch 00020: val_loss did not improve\n",
      "5406/5406 [==============================] - 0s - loss: 0.2214 - val_loss: 0.3456\n",
      "Epoch 22/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2307Epoch 00021: val_loss did not improve\n",
      "5406/5406 [==============================] - 0s - loss: 0.2278 - val_loss: 0.3052\n",
      "Epoch 23/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2174\n",
      "Epoch 00022: reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 00022: val_loss did not improve\n",
      "5406/5406 [==============================] - 1s - loss: 0.2165 - val_loss: 0.3204\n",
      "Epoch 24/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2143Epoch 00023: val_loss did not improve\n",
      "5406/5406 [==============================] - 0s - loss: 0.2147 - val_loss: 0.3080\n",
      "Epoch 25/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2165Epoch 00024: val_loss did not improve\n",
      "5406/5406 [==============================] - 0s - loss: 0.2174 - val_loss: 0.3051\n",
      "Epoch 26/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2061Epoch 00025: val_loss did not improve\n",
      "5406/5406 [==============================] - 0s - loss: 0.2062 - val_loss: 0.3109\n",
      "Epoch 27/300\n",
      "5000/5406 [==========================>...] - ETA: 0s - loss: 0.2085Epoch 00026: val_loss did not improve\n",
      "5406/5406 [==============================] - 0s - loss: 0.2095 - val_loss: 0.3090\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, min_lr=1e-7, verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath=model_name+'.h5', verbose=1, save_best_only=True)\n",
    "EarlStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=12, verbose=0, mode='auto')\n",
    "\n",
    "history = model.fit(X_input_train, Y_input_train, \n",
    "                      epochs = 300, \n",
    "                      batch_size = batch_size, \n",
    "                      verbose=1, \n",
    "                      validation_split = 0.3,\n",
    "                      #validation_data=(X_input_cv, Y_input_cv),\n",
    "                      callbacks=[reduce_lr, checkpointer, EarlStop],\n",
    "                      shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00       124\n",
      "          1       1.00      0.98      0.99       353\n",
      "          2       0.21      0.12      0.15       329\n",
      "          3       0.31      0.37      0.34       172\n",
      "          4       0.98      0.99      0.99       186\n",
      "          5       1.00      0.98      0.99       129\n",
      "          6       0.40      0.70      0.51       164\n",
      "          7       0.99      1.00      1.00       179\n",
      "          8       0.99      1.00      0.99       175\n",
      "\n",
      "avg / total       0.73      0.75      0.73      1811\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred = np.empty_like(Y_input_test)\n",
    "for i in range(0,Y_pred.shape[0]):\n",
    "    Y_pred[i,:] = model.predict(x = X_input_test[[i],:,:])\n",
    "\n",
    "Y_pred_1d = pcnp.to_1d_labels(Y_pred)\n",
    "rep = classification_report(Y_test_dl.astype(int),Y_pred_1d)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[123   0   0   1   0   0   0   0   0]\n",
      " [  0   0   0   0 184   1   1   0   0]\n",
      " [  0   0   0   1   0 128   0   0   0]\n",
      " [  0   1  31 144   0   0 153   0   0]\n",
      " [  0   0   0   0   0   0   0   0 175]\n",
      " [  0 349   0   2   2   0   0   0   0]\n",
      " [  0   0  77   4   0   0  83   0   0]\n",
      " [  0   0   0   0   0   0   0 178   1]\n",
      " [  0   0  77  92   1   1   1   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_test.astype(int), Y_pred_1d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
